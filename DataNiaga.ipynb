{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7H9Xv88m6bsh",
      "metadata": {
        "id": "7H9Xv88m6bsh"
      },
      "outputs": [],
      "source": [
        "import os, json, kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qmaCNbU6cAi",
      "metadata": {
        "id": "7qmaCNbU6cAi"
      },
      "outputs": [],
      "source": [
        "# creds = {}\n",
        "# creds[\"username\"] = \"danishrme\"\n",
        "# creds[\"key\"] = \"877fbf0b79d7c37e5a1a3c8552f3a13e\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46bd46ac",
      "metadata": {
        "id": "46bd46ac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f233b37e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f233b37e",
        "outputId": "a394376d-c3bc-4d4f-d855-1789d8153ad8"
      },
      "outputs": [],
      "source": [
        "# df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, \"danishrme/data-retail\",\"data_retail2.xlsx\")\n",
        "df = pd.read_excel(\"data_retail2.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a7b627",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "start_date = pd.to_datetime(\"2022-01-02\")\n",
        "target_date = pd.to_datetime(\"2025-12-14\")\n",
        "\n",
        "time_delta = target_date - start_date\n",
        "df['InvoiceDate'] = df['InvoiceDate'] + time_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3faad91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "e3faad91",
        "outputId": "a31983d8-ed2d-4156-f8d2-c6dda2e5d088"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8dfc3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5ee6b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5ee6b2",
        "outputId": "0d42a1e3-dac3-4620-c0e9-606da1476ff0"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270b46f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "270b46f1",
        "outputId": "5d61f907-36f9-4151-d52a-c63e580faf98"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d40226",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "50d40226",
        "outputId": "a72a8180-2c88-4e2a-d344-2657afda8c42"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5253b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a5253b9",
        "outputId": "3d757ead-5520-467d-893a-090c00ac6e70"
      },
      "outputs": [],
      "source": [
        "df['InvoiceNo'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31d6c64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f31d6c64",
        "outputId": "87fa45a2-ad2d-46b3-9696-ee7c0195da54"
      },
      "outputs": [],
      "source": [
        "obj_col = df.select_dtypes(include='object').columns\n",
        "for col in obj_col:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c096f9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c096f9b",
        "outputId": "0aaba6a2-839b-4b42-b737-b976ec9035b3"
      },
      "outputs": [],
      "source": [
        "df['BRANCH_SPLR'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3971961",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3971961",
        "outputId": "cc1be146-3d7c-43ed-dfc9-ae2a01210549"
      },
      "outputs": [],
      "source": [
        "df['BRANCH_SPLR'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4ee0c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f4ee0c4",
        "outputId": "e80beccb-3e7a-4b79-a369-2cfaaaeab3d8"
      },
      "outputs": [],
      "source": [
        "df['warehouseProductsID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def58dea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def58dea",
        "outputId": "1d06efb3-d864-43b4-81d6-e6fec93bf050"
      },
      "outputs": [],
      "source": [
        "df['warehouseProductsID'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11479f19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "11479f19",
        "outputId": "e0920680-b2e9-4075-9d11-e210dce80347"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa87c506",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa87c506",
        "outputId": "ec0bc799-7cba-4f0b-80a8-8605211e34a9"
      },
      "outputs": [],
      "source": [
        "df['CHANNELID_SPLR'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969f9aa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "969f9aa2",
        "outputId": "0eff8c22-fc62-40a3-d336-f3449c641045"
      },
      "outputs": [],
      "source": [
        "df['CHANNELNAME_SPLR'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8412f38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "f8412f38",
        "outputId": "f0eaa953-81e7-4fbd-957c-71e390f11c40"
      },
      "outputs": [],
      "source": [
        "df[df['CHANNELNAME_SPLR'] == 'Salon Kecantikan']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340ebf56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "340ebf56",
        "outputId": "da223911-f6ef-4858-e209-c1daabf83005"
      },
      "outputs": [],
      "source": [
        "df['CHANNELNAME_SPLR'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df777233",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "df777233",
        "outputId": "16c6f9d9-659a-4ded-b6b9-56557701632d"
      },
      "outputs": [],
      "source": [
        "df[df['InvoiceNo'] == 536365].drop(['PRODUCT_CATEGORY', 'StockCode', 'UnitPrice'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9c6ad3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9c6ad3",
        "outputId": "b5de6fd2-b091-40ab-d804-9092056b3e25"
      },
      "outputs": [],
      "source": [
        "df['CustomerID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f46271a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f46271a9",
        "outputId": "74c9a3c7-54d3-4d09-ee8e-96759bac3c99"
      },
      "outputs": [],
      "source": [
        "df['oldCUSTID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a26b36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c8a26b36",
        "outputId": "31fd0126-6617-4b2a-bf98-ff0b4fe6bc18"
      },
      "outputs": [],
      "source": [
        "df[df['InvoiceNo'] == 581000].drop(['PRODUCT_CATEGORY', 'StockCode', 'UnitPrice'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002645cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002645cb",
        "outputId": "c0b96029-bf59-42b4-e961-c6c2a6100a0e"
      },
      "outputs": [],
      "source": [
        "df['PRODUCT'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3961afc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3961afc",
        "outputId": "20b2313a-cd29-438a-ebbc-d357b4db7c2c"
      },
      "outputs": [],
      "source": [
        "df['PRODUCT_CATEGORY'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edb0714",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9edb0714",
        "outputId": "ccf74839-db81-4017-88bd-7883082ce36c"
      },
      "outputs": [],
      "source": [
        "df['PRODUCT_CATEGORY'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ae40cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d5ae40cd",
        "outputId": "71ac5817-7d75-4eba-b9dd-b7aa96a04dea"
      },
      "outputs": [],
      "source": [
        "df[df['PRODUCT_CATEGORY'] == 'KOSMETIK']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e182e20c",
      "metadata": {
        "id": "e182e20c"
      },
      "source": [
        "ada yang miss labeling anjing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250748da",
      "metadata": {
        "id": "250748da"
      },
      "outputs": [],
      "source": [
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec3d180",
      "metadata": {
        "id": "4ec3d180"
      },
      "outputs": [],
      "source": [
        "df['InvoiceDate'] = df['InvoiceDate'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7835c63a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7835c63a",
        "outputId": "059258af-c2e2-454f-ed76-8900578f7ff1"
      },
      "outputs": [],
      "source": [
        "df['PROVINSI'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f8bd38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d2f8bd38",
        "outputId": "2becca50-c7c4-4e72-b7ee-15b7c6c63dfb"
      },
      "outputs": [],
      "source": [
        "df[df['PROVINSI'].isna()]['KOTA'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931578fd",
      "metadata": {
        "id": "931578fd"
      },
      "outputs": [],
      "source": [
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SOLO 08960358885', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'NGUNTORONADI', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'JATIROTO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'MERBAUNG-KLATEN SELA', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BATURETNO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'PRMBNAN KLT', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'NGAWEN KLATEN', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BULIKERTO WONOGIRI', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'KARTASURA', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'NGADIROJO WONOGIRI', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'TIRTOMOYO WONOGIRI', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'MOJOSONGO SURAKARTA', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'KLATEN UTARA', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'POLOKERTO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'KARTOSURO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SURAKARTA', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SUKOHARJO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'KARANGANYAR', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BOYOLALI', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SOLO', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SANGATA', 'PROVINSI'] = 'KALIMANTAN TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SRAGEN', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BONDOWOSO', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'WONOGIRI', 'PROVINSI'] = 'JAWA TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SITUBONDO', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'JEMBER', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BANYUWANGI', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'SAMPIT', 'PROVINSI'] = 'KALIMANTAN TENGAH'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'BONTANG', 'PROVINSI'] = 'KALIMANTAN TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'LUMAJANG', 'PROVINSI'] = 'JAWA TIMUR'\n",
        "df.loc[df['KOTA'].astype(str).str.strip().str.upper() == 'KLATEN', 'PROVINSI'] = 'JAWA TENGAH'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c3d906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "f2c3d906",
        "outputId": "aaf69d59-1137-4884-b034-b400c91dce05"
      },
      "outputs": [],
      "source": [
        "df[df['KOTA'].isna() & df['PROVINSI'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0cc92be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "d0cc92be",
        "outputId": "7843862d-5520-452b-80f7-01a872a5ee9e"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bccc875",
      "metadata": {
        "id": "2bccc875"
      },
      "outputs": [],
      "source": [
        "df = df[df['KOTA'].notna() | df['PROVINSI'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e23122",
      "metadata": {
        "id": "b4e23122"
      },
      "outputs": [],
      "source": [
        "df['PROVINSI'] = df['PROVINSI'].str.strip().str.upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "144150cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "144150cd",
        "outputId": "d0e52dd5-dd1b-40f0-941d-9955032ac99d"
      },
      "outputs": [],
      "source": [
        "df['PROVINSI'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9633fa8b",
      "metadata": {
        "id": "9633fa8b"
      },
      "outputs": [],
      "source": [
        "df['PROVINSI'] = df['PROVINSI'].replace('KAL-BAR', 'KALIMANTAN BARAT')\n",
        "df['PROVINSI'] = df['PROVINSI'].replace('KAL-SEL', 'KALIMANTAN SELATAN')\n",
        "df['PROVINSI'] = df['PROVINSI'].replace('KALTENG', 'KALIMANTAN TENGAH')\n",
        "df['PROVINSI'] = df['PROVINSI'].replace('JAWATENGAH', 'JAWA TENGAH')\n",
        "df['PROVINSI'] = df['PROVINSI'].replace('DIY', 'DI YOGYAKARTA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a94834",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51a94834",
        "outputId": "1e86317a-474b-4f42-b17c-5593abed35e7"
      },
      "outputs": [],
      "source": [
        "grouped = df.groupby(['PROVINSI', 'PRODUCT_CATEGORY'], dropna=False)\n",
        "nested_dict = {}\n",
        "\n",
        "for (provinsi, product_cat), group in grouped:\n",
        "    provinsi_key = provinsi if pd.notna(provinsi) else 'Unknown'\n",
        "    group_sorted = group.sort_values('InvoiceDate').reset_index(drop=True)\n",
        "    nested_dict.setdefault(provinsi_key, {})[product_cat] = group_sorted.copy()\n",
        "\n",
        "print(f\"✅ Created nested_dict with {len(nested_dict)} PROVINSI keys and \"\n",
        "      f\"{sum(len(v) for v in nested_dict.values())} (PROVINSI, PRODUCT_CATEGORY) groups\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ce6e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ce6e1b",
        "outputId": "02ba0583-6625-4d91-9b88-4c0fe7f97941"
      },
      "outputs": [],
      "source": [
        "df['InvoiceDate'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3dba6c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3dba6c8",
        "outputId": "d1d0b67f-3712-48bb-c2cc-c57aa21565d0"
      },
      "outputs": [],
      "source": [
        "df['InvoiceDate'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99921396",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99921396",
        "outputId": "4bec35d9-ff89-41d6-f4b5-36a8e0aa4932"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "start = datetime.strptime('2020-12-01', '%Y-%m-%d').date()\n",
        "end = datetime.strptime('2021-12-09', '%Y-%m-%d').date()\n",
        "all_dates = set([start + timedelta(days=x) for x in range((end-start).days + 1)])\n",
        "\n",
        "for provinsi in nested_dict.keys():\n",
        "    print(f\"Provinsi: {provinsi}\")\n",
        "    for product_cat in nested_dict[provinsi].keys():\n",
        "        existing_dates = set(nested_dict[provinsi][product_cat]['InvoiceDate'])\n",
        "        missing_dates = all_dates - existing_dates\n",
        "\n",
        "        print(f\"\\t{product_cat} : {len(missing_dates)} rows\")\n",
        "        # if len(missing_dates) > 0:\n",
        "        #     print(\"Sample of missing dates (first 5):\")\n",
        "        #     print(sorted(list(missing_dates))[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f051c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18f051c5",
        "outputId": "8e4dfe01-46ca-49af-d3a4-e7d31030dc61"
      },
      "outputs": [],
      "source": [
        "nested_dict['BALI']['ALAT LISTRIK']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048ac97b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "048ac97b",
        "outputId": "f11ba7b3-6439-4c41-9ff7-3e64faf3f192"
      },
      "outputs": [],
      "source": [
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f25ea0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4f25ea0",
        "outputId": "6dc2b7b3-2a0c-4df5-8dbe-7701527d974c"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6f1c9a",
      "metadata": {
        "id": "3c6f1c9a"
      },
      "outputs": [],
      "source": [
        "df['Quantity'] = df['Quantity'].apply(lambda x : -x if x < 0 else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd9b6af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcd9b6af",
        "outputId": "63d2da83-184f-437d-a7dd-8c1ce776b2c5"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'InvoiceNo', 'BRANCH_SPLR', 'BRANCHNAME_SPLR',\n",
        "    'warehouseProductsID', 'BARCODEID', 'StockCode', 'PRODUCT',\n",
        "    'PRODUCT_CATEGORY', 'UnitPrice',\n",
        "    'oldCUSTID', 'CustomerID', 'CUSTNAME', 'ADDRESS', 'KOTA', 'PROVINSI',\n",
        "    'NEGARA', 'CHANNELID_SPLR', 'CHANNELNAME_SPLR', 'SUBDISTID', 'SUBDIST_NAME'\n",
        "]\n",
        "\n",
        "df_temp = df.copy()\n",
        "for provinsi, categories in nested_dict.items():\n",
        "    for product_cat, df_temp in categories.items():\n",
        "        df_temp = df_temp.drop(columns=cols_to_drop, errors='ignore')\n",
        "        df_temp['InvoiceDate'] = pd.to_datetime(df_temp['InvoiceDate'])\n",
        "\n",
        "        grouped_df = (\n",
        "                df_temp.groupby('InvoiceDate', as_index=False)[['Quantity', 'UnitPriceRupiah']]\n",
        "              .sum()\n",
        "              .sort_values('InvoiceDate')\n",
        "              .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        nested_dict[provinsi][product_cat] = grouped_df\n",
        "\n",
        "print(\"✅ Grouping and aggregation by date completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac84aa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cac84aa1",
        "outputId": "2e98d253-7f77-4c4f-b268-45945fa726ef"
      },
      "outputs": [],
      "source": [
        "nested_dict['BALI']['ALAT RUMAH TANGGA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb32921",
      "metadata": {
        "id": "fcb32921"
      },
      "outputs": [],
      "source": [
        "# start = datetime.strptime('2020-12-01', '%Y-%m-%d').date()\n",
        "# end = datetime.strptime('2021-12-09', '%Y-%m-%d').date()\n",
        "# all_dates = pd.date_range(start=start, end=end)\n",
        "\n",
        "# for provinsi, categories in nested_dict.items():\n",
        "#     print(f\"Provinsi: {provinsi}\")\n",
        "#     for product_cat, df in categories.items():\n",
        "#         df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']).dt.date\n",
        "\n",
        "#         existing_dates = set(df['InvoiceDate'])\n",
        "#         missing_dates = set(all_dates.date) - existing_dates\n",
        "\n",
        "#         print(f\"\\t{product_cat}: {len(missing_dates)} missing rows\")\n",
        "\n",
        "#         if missing_dates:\n",
        "#             imputed_df = pd.DataFrame({\n",
        "#                 'InvoiceDate': sorted(missing_dates),\n",
        "#                 'Quantity': 0,\n",
        "#                 'UnitPriceRupiah': 0\n",
        "#             })\n",
        "\n",
        "#             df_full = pd.concat([df, imputed_df], ignore_index=True)\n",
        "#             df_full = df_full.sort_values('InvoiceDate').reset_index(drop=True)\n",
        "#             nested_dict[provinsi][product_cat] = df_full\n",
        "#         else:\n",
        "#             nested_dict[provinsi][product_cat] = df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65525f2c",
      "metadata": {
        "id": "65525f2c"
      },
      "outputs": [],
      "source": [
        "# plt.style.use('seaborn-v0_8')\n",
        "# plt.figure(figsize=(15, 10))\n",
        "\n",
        "# for provinsi, categories in nested_dict.items():\n",
        "#     plt.figure(figsize=(15, 10))\n",
        "\n",
        "#     for product_cat, df in categories.items():\n",
        "#         plt.plot(df['InvoiceDate'], df['Quantity'], label=product_cat)\n",
        "\n",
        "#     plt.title(f'Daily Sales Quantity in {provinsi}')\n",
        "#     plt.xlabel('Date')\n",
        "#     plt.ylabel('Quantity')\n",
        "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     plt.xticks(rotation=45)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006d63db",
      "metadata": {
        "id": "006d63db"
      },
      "outputs": [],
      "source": [
        "# plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# for provinsi, categories in nested_dict.items():\n",
        "#     plt.figure(figsize=(15, 10))\n",
        "\n",
        "#     for product_cat, df in categories.items():\n",
        "#         df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "#         monthly_sum = (\n",
        "#             df.groupby(pd.Grouper(key='InvoiceDate', freq='M'))['Quantity']\n",
        "#               .sum()\n",
        "#               .reset_index()\n",
        "#         )\n",
        "\n",
        "#         plt.plot(monthly_sum['InvoiceDate'], monthly_sum['Quantity'], label=product_cat)\n",
        "\n",
        "#     plt.title(f\"Monthly Sales Quantity in {provinsi}\")\n",
        "#     plt.xlabel(\"Month\")\n",
        "#     plt.ylabel(\"Total Quantity Sold\")\n",
        "#     plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     plt.xticks(rotation=45)\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a4351c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3a4351c",
        "outputId": "fbfccba8-35c6-44d5-dc2f-734f9f80836f"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "categories = sorted(df['PRODUCT_CATEGORY'].unique())\n",
        "all_dates = pd.date_range(start=pd.to_datetime(start), end=pd.to_datetime(end))\n",
        "\n",
        "for product_cat in categories:\n",
        "    tmp = df[df['PRODUCT_CATEGORY'] == product_cat].copy()\n",
        "\n",
        "    if tmp.empty:\n",
        "        continue\n",
        "\n",
        "    tmp['InvoiceDate'] = pd.to_datetime(tmp['InvoiceDate'])\n",
        "\n",
        "    # A. DATA HARIAN\n",
        "    daily = tmp.groupby(tmp['InvoiceDate'].dt.date)['Quantity'].sum().reset_index()\n",
        "    daily.columns = ['InvoiceDate', 'qty_sum']\n",
        "    daily.index = pd.to_datetime(daily['InvoiceDate'])\n",
        "    daily_full = daily.reindex(all_dates, method=None)\n",
        "\n",
        "    # B. DATA BULANAN\n",
        "    monthly = tmp.groupby(pd.Grouper(key='InvoiceDate', freq='M'))['Quantity'].sum()\n",
        "\n",
        "    # C. DATA MINGGUAN\n",
        "    weekly = tmp.groupby(pd.Grouper(key='InvoiceDate', freq='W'))['Quantity'].sum()\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "    # Daily\n",
        "    axes[0].plot(daily_full.index, daily_full['qty_sum'], color='tab:blue', linestyle='-', linewidth=1)\n",
        "    axes[0].set_title(f'Daily Quantity: {product_cat}', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Date')\n",
        "    axes[0].set_ylabel('Quantity')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # Monthly\n",
        "    axes[1].plot(monthly.index, monthly.values, color='tab:red', marker='o', linestyle='-', linewidth=2)\n",
        "    axes[1].set_title(f'Monthly Quantity: {product_cat}', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Date')\n",
        "    axes[1].set_ylabel('Quantity')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    # Weekly\n",
        "    axes[2].plot(weekly.index, weekly.values, color='tab:green', marker='s', linestyle='-', linewidth=2)\n",
        "    axes[2].set_title(f'Weekly Quantity: {product_cat}', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_xlabel('Date')\n",
        "    axes[2].set_ylabel('Quantity')\n",
        "    axes[2].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b96c65d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "b96c65d5",
        "outputId": "72ff54ba-5258-40d5-e0a0-cfda88a1e621"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cabe9c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cabe9c8",
        "outputId": "45f7ee8c-437b-4026-ca9e-67b1b98ae4e5"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ede3022",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ede3022",
        "outputId": "7fa77f7a-9da7-49d1-df2d-7de93235cea6"
      },
      "outputs": [],
      "source": [
        "df['PROVINSI'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767d34bb",
      "metadata": {
        "id": "767d34bb"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['InvoiceNo', 'BRANCH_SPLR', 'BRANCHNAME_SPLR',\n",
        "    'warehouseProductsID', 'BARCODEID', 'StockCode', 'PRODUCT', 'UnitPrice', 'oldCUSTID',\n",
        "    'CustomerID', 'CUSTNAME', 'ADDRESS', 'KOTA', 'PROVINSI', 'NEGARA',\n",
        "    'CHANNELID_SPLR', 'CHANNELNAME_SPLR', 'SUBDISTID', 'SUBDIST_NAME', 'UnitPriceRupiah'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efef1897",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "efef1897",
        "outputId": "e4617959-d0b7-4b64-aaa3-72e1ffd4ddb9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893b78c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "893b78c3",
        "outputId": "669623d9-89ae-4de4-907a-262c9643c784"
      },
      "outputs": [],
      "source": [
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='ME'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('ME').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "test_months = 3\n",
        "train_data = df_pivot.iloc[:-test_months]\n",
        "test_data = df_pivot.iloc[-test_months:]\n",
        "\n",
        "print(f\"Jumlah Bulan Training: {len(train_data)}\")\n",
        "print(f\"Jumlah Bulan Testing : {len(test_data)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    y_train = train_data[category]\n",
        "    y_test = test_data[category]\n",
        "\n",
        "    try:\n",
        "        model = ExponentialSmoothing(\n",
        "            y_train,\n",
        "            trend='add',\n",
        "            seasonal=None,\n",
        "            initialization_method=\"estimated\"\n",
        "        ).fit()\n",
        "\n",
        "        forecast = model.forecast(steps=len(y_test))\n",
        "        forecast = pd.Series(forecast, index=y_test.index)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
        "        mape = mean_absolute_percentage_error(y_test, forecast)\n",
        "\n",
        "        evaluation_results.append({\n",
        "            'Category': category,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'Last_Actual': y_test.iloc[-1],\n",
        "            'Last_Pred': forecast.iloc[-1]\n",
        "        })\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        plt.subplot(int(np.ceil(len(df_pivot.columns)/3)), 3, i+1)\n",
        "\n",
        "        plt.plot(y_train.index, y_train, label='Train', color='blue', linewidth=1.5)\n",
        "        plt.plot(y_test.index, y_test, label='Test (Actual)', color='green', linewidth=1.5)\n",
        "        plt.plot(forecast.index, forecast, label='Forecast', color='red', linestyle='--', linewidth=1.5)\n",
        "\n",
        "        plt.title(f\"{category}\\nMAPE: {mape:.2%}\", fontsize=9, pad=10)\n",
        "        plt.xlabel('Date', fontsize=8)\n",
        "        plt.ylabel('Quantity', fontsize=8)\n",
        "        plt.xticks(rotation=45, fontsize=7)\n",
        "        plt.yticks(fontsize=7)\n",
        "        plt.grid(alpha=0.3)\n",
        "        if i == 0:\n",
        "            plt.legend(fontsize=8, loc='best')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memproses kategori {category}: {e}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. PRINT HASIL EVALUASI ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== TOP 5 BEST MODELS (SARIMA) ===\")\n",
        "print(df_eval[['Category', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage').tail(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d733629",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0d733629",
        "outputId": "2398b276-1328-4658-ca58-db36474d0238"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import warnings\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# Matikan warning agar output bersih saat Grid Search\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- 1. DATA PREPARATION (WEEKLY) ---\n",
        "# (Sama seperti sebelumnya)\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- 2. SETUP DATA ---\n",
        "test_weeks = 12\n",
        "train_data = df_pivot.iloc[:-test_weeks]\n",
        "test_data = df_pivot.iloc[-test_weeks:]\n",
        "\n",
        "print(f\"Data Training: {len(train_data)} minggu\")\n",
        "print(f\"Data Testing : {len(test_data)} minggu\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "# --- SETUP GRID SEARCH PARAMETERS ---\n",
        "# Kita buat kombinasi parameter p, d, q sederhana (0 atau 1) agar tidak terlalu lama\n",
        "# p = AutoRegressive, d = Integrated (Difference), q = Moving Average\n",
        "p = d = q = range(0, 2)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "\n",
        "# --- 3. LOOPING SMART MODELING ---\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(df_pivot.columns) / n_cols))\n",
        "\n",
        "plt.figure(figsize=(18, 5 * n_rows))\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    y_train = train_data[category]\n",
        "    y_test = test_data[category]\n",
        "\n",
        "    # --- AUTO-ARIMA LOGIC (Grid Search) ---\n",
        "    best_aic = float(\"inf\")\n",
        "    best_param = None\n",
        "    best_model_fit = None\n",
        "\n",
        "    print(f\"Training Category: {category}...\", end=\" \")\n",
        "\n",
        "    # Mencari parameter terbaik untuk kategori ini\n",
        "    for param in pdq:\n",
        "        try:\n",
        "            # Menggunakan SARIMAX\n",
        "            # enforce_stationarity=False agar lebih fleksibel terhadap data tren\n",
        "            temp_model = SARIMAX(y_train,\n",
        "                                order=param,\n",
        "                                seasonal_order=(0, 0, 0, 0), # Set seasonal jika data > 2 tahun\n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "            results = temp_model.fit(disp=False)\n",
        "\n",
        "            # Kita cari AIC terkecil (Model paling efisien)\n",
        "            if results.aic < best_aic:\n",
        "                best_aic = results.aic\n",
        "                best_param = param\n",
        "                best_model_fit = results\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"Best Order: {best_param} | AIC: {best_aic:.2f}\")\n",
        "\n",
        "    # --- FORECASTING ---\n",
        "    if best_model_fit is not None:\n",
        "        # Forecast langkah ke depan\n",
        "        forecast_obj = best_model_fit.get_forecast(steps=len(y_test))\n",
        "        forecast = forecast_obj.predicted_mean\n",
        "        forecast.index = y_test.index\n",
        "\n",
        "        # Interval Kepercayaan (Confidence Interval) - Fitur Keren SARIMA\n",
        "        # Ini memberikan area 'arsiran' kemungkinan prediksi\n",
        "        conf_int = forecast_obj.conf_int(alpha=0.05)\n",
        "\n",
        "        # --- EVALUASI ---\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
        "        mape = mean_absolute_percentage_error(y_test, forecast)\n",
        "\n",
        "        evaluation_results.append({\n",
        "            'Category': category,\n",
        "            'Model_Order': str(best_param),\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'Last_Actual': y_test.iloc[-1],\n",
        "            'Last_Pred': forecast.iloc[-1]\n",
        "        })\n",
        "\n",
        "        # --- PLOTTING ---\n",
        "        plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "        # Plot Data\n",
        "        plt.plot(y_train.index, y_train, label='Train', color='tab:blue')\n",
        "        plt.plot(y_test.index, y_test, label='Test (Actual)', color='tab:green')\n",
        "        plt.plot(forecast.index, forecast, label=f'SARIMA {best_param}', color='tab:red', linestyle='--', linewidth=2)\n",
        "\n",
        "        # Plot Confidence Interval (Area Arsiran Abu-abu)\n",
        "        plt.fill_between(conf_int.index,\n",
        "                         conf_int.iloc[:, 0],\n",
        "                         conf_int.iloc[:, 1], color='pink', alpha=0.3, label='95% Conf. Int.')\n",
        "\n",
        "        plt.title(f\"{category}\\nMAPE: {mape:.2%} (Order: {best_param})\", fontsize=10, fontweight='bold')\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=45, fontsize=8)\n",
        "\n",
        "        if i == 0: plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "    else:\n",
        "        print(f\"  -> Gagal menemukan model konvergen untuk {category}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. PRINT HASIL EVALUASI ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== TOP 5 BEST MODELS (SARIMA) ===\")\n",
        "print(df_eval[['Category', 'Model_Order', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage').head(53))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YSEbk15wDWph",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YSEbk15wDWph",
        "outputId": "774a6002-520f-44e5-e519-b8c0a5e650d7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "import logging\n",
        "\n",
        "# Matikan log warning Prophet agar output bersih\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
        "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
        "\n",
        "# --- 1. DATA PREPARATION (WEEKLY) ---\n",
        "# Pastikan InvoiceDate datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Aggregasi Mingguan (W)\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "\n",
        "# Resample & Interpolate (Prophet sebenarnya bisa handle missing, tapi interpolasi bikin lebih mulus)\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "\n",
        "# Stack\n",
        "df_clean = df_interpolated.stack().reset_index()\n",
        "df_clean.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "\n",
        "# Pivot untuk memudahkan looping\n",
        "df_pivot = df_clean.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- 2. SETUP TESTING ---\n",
        "test_weeks = 12\n",
        "# Di Prophet, splitting agak beda, kita akan split dataframe per kategori nanti di dalam loop\n",
        "print(f\"Menggunakan Prophet untuk {len(df_pivot.columns)} Kategori...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "# --- 3. LOOPING PROPHET ---\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(df_pivot.columns) / n_cols))\n",
        "\n",
        "plt.figure(figsize=(18, 5 * n_rows))\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    # A. SIAPKAN DATA FORMAT PROPHET (ds, y)\n",
        "    # Ambil data kategori spesifik\n",
        "    df_cat = df_clean[df_clean['PRODUCT_CATEGORY'] == category].copy()\n",
        "\n",
        "    # Rename wajib untuk Prophet\n",
        "    df_cat = df_cat.rename(columns={'InvoiceDate': 'ds', 'Quantity': 'y'})\n",
        "\n",
        "    # Split Train & Test\n",
        "    train_prophet = df_cat.iloc[:-test_weeks]\n",
        "    test_prophet = df_cat.iloc[-test_weeks:]\n",
        "\n",
        "    try:\n",
        "        # B. DEFINE & FIT MODEL\n",
        "        # daily_seasonality=False (karena data mingguan)\n",
        "        # weekly_seasonality=False (karena 1 titik per minggu, tidak ada pola 'senin vs minggu')\n",
        "        # yearly_seasonality=True (mencoba menangkap pola tahunan, meski data mepet)\n",
        "        model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            # yearly_seasonality=True,\n",
        "            changepoint_prior_scale=0.05 # Fleksibilitas tren (default 0.05). Naikkan jika tren berubah cepat.\n",
        "        )\n",
        "\n",
        "        model.fit(train_prophet)\n",
        "\n",
        "        # C. PREDIKSI\n",
        "        # Buat dataframe masa depan untuk menampung hasil prediksi\n",
        "        # periods = jumlah minggu test\n",
        "        future = model.make_future_dataframe(periods=len(test_prophet), freq='W')\n",
        "        forecast = model.predict(future)\n",
        "\n",
        "        # D. AMBIL DATA PREDIKSI SAJA (BAGIAN TEST)\n",
        "        # Forecast dataframe berisi sejarah + masa depan. Kita ambil ekornya saja.\n",
        "        y_pred = forecast['yhat'].iloc[-test_weeks:]\n",
        "        y_pred_lower = forecast['yhat_lower'].iloc[-test_weeks:] # Batas bawah\n",
        "        y_pred_upper = forecast['yhat_upper'].iloc[-test_weeks:] # Batas atas\n",
        "\n",
        "        # Pastikan index sama untuk kalkulasi\n",
        "        y_true = test_prophet['y'].values\n",
        "\n",
        "        # E. EVALUASI\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "        evaluation_results.append({\n",
        "            'Category': category,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'Last_Actual': y_true[-1],\n",
        "            'Last_Pred': y_pred.values[-1]\n",
        "        })\n",
        "\n",
        "        # F. PLOTTING\n",
        "        plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "        # Plot Train\n",
        "        plt.plot(train_prophet['ds'], train_prophet['y'], label='Train', color='tab:blue')\n",
        "        # Plot Test (Actual)\n",
        "        plt.plot(test_prophet['ds'], test_prophet['y'], label='Test (Actual)', color='tab:green')\n",
        "        # Plot Forecast (Predicted)\n",
        "        plt.plot(test_prophet['ds'], y_pred.values, label='Prophet Forecast', color='tab:red', linestyle='--', linewidth=2)\n",
        "\n",
        "        # Plot Confidence Interval (Prophet Uncertainty)\n",
        "        plt.fill_between(test_prophet['ds'], y_pred_lower, y_pred_upper, color='pink', alpha=0.4, label='Uncertainty Interval')\n",
        "\n",
        "        plt.title(f\"{category}\\nMAPE: {mape:.2%}\", fontsize=11, fontweight='bold')\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.xticks(rotation=45, fontsize=8)\n",
        "\n",
        "        if i == 0: plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal Prophet untuk {category}: {e}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4. HASIL EVALUASI ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== TOP 5 BEST PROPHET RESULTS ===\")\n",
        "print(df_eval[['Category', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage').head(53))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nDgphqFxECgx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nDgphqFxECgx",
        "outputId": "86978bd7-166d-4495-8fe2-5a97377145e6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# --- 1. DATA PREPARATION (Sama seperti sebelumnya) ---\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "test_weeks = 12\n",
        "evaluation_results = []\n",
        "\n",
        "print(f\"Menggunakan XGBoost (Recursive) untuk {len(df_pivot.columns)} Kategori...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 3. HELPER FUNCTION: FEATURE ENGINEERING ---\n",
        "def create_features(df_input, label=None):\n",
        "    \"\"\"\n",
        "    Membuat fitur waktu & lag dari dataframe single series\n",
        "    \"\"\"\n",
        "    df_input = df_input.copy()\n",
        "    # Fitur Waktu\n",
        "    df_input['week_of_year'] = df_input.index.isocalendar().week.astype(int)\n",
        "    df_input['month'] = df_input.index.month\n",
        "\n",
        "    # Fitur Lag (Nilai masa lalu)\n",
        "    # Kita pakai Lag 1 (minggu lalu) sampai Lag 4 (sebulan lalu)\n",
        "    for lag in [1, 2, 3, 4]:\n",
        "        df_input[f'lag_{lag}'] = df_input['Quantity'].shift(lag)\n",
        "\n",
        "    return df_input\n",
        "\n",
        "# --- 4. MAIN LOOP ---\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(df_pivot.columns) / n_cols))\n",
        "\n",
        "plt.figure(figsize=(18, 5 * n_rows))\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    # Ambil data per kategori\n",
        "    # Kita buat DataFrame khusus dengan index tanggal\n",
        "    temp_df = df_pivot[[category]].rename(columns={category: 'Quantity'})\n",
        "\n",
        "    # --- SPLIT TRAIN & TEST ---\n",
        "    # Kita pisah DULU sebelum feature engineering untuk menghindari data leakage (kebocoran masa depan)\n",
        "    train_raw = temp_df.iloc[:-test_weeks].copy()\n",
        "    test_raw = temp_df.iloc[-test_weeks:].copy()\n",
        "\n",
        "    # --- TRAINING PHASE ---\n",
        "    # Buat fitur pada data training\n",
        "    train_features = create_features(train_raw)\n",
        "\n",
        "    # Hapus NaN akibat Lag (4 baris pertama pasti NaN karena shift)\n",
        "    train_features = train_features.dropna()\n",
        "\n",
        "    FEATURES = ['week_of_year', 'month', 'lag_1', 'lag_2', 'lag_3', 'lag_4']\n",
        "    TARGET = 'Quantity'\n",
        "\n",
        "    X_train = train_features[FEATURES]\n",
        "    y_train = train_features[TARGET]\n",
        "\n",
        "    # Define Model XGBoost\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3, # Keep it simple agar tidak overfitting di data kecil\n",
        "        early_stopping_rounds=50,\n",
        "        objective='reg:squarederror',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit Model (Pakai eval set agar berhenti jika tidak ada improvement)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_train, y_train)],\n",
        "              verbose=False)\n",
        "\n",
        "    # --- RECURSIVE FORECASTING PHASE ---\n",
        "    # Karena kita butuh Lag, kita tidak bisa langsung predict X_test.\n",
        "    # Kita harus predict satu per satu, lalu update Lag untuk prediksi berikutnya.\n",
        "\n",
        "    # Ambil data terakhir dari training sebagai basis awal\n",
        "    current_input_data = train_raw.copy()\n",
        "    predictions = []\n",
        "\n",
        "    # Loop untuk setiap minggu di masa depan (Test set)\n",
        "    for week in range(len(test_raw)):\n",
        "        # 1. Buat fitur untuk seluruh data gabungan (History + Prediksi sejauh ini)\n",
        "        #    Kita butuh kalkulasi ulang create_features di setiap langkah agar Lag-nya update\n",
        "        full_features = create_features(current_input_data)\n",
        "\n",
        "        # 2. Ambil baris terakhir (ini adalah \"Besok\" yang mau kita tebak)\n",
        "        #    Sebenarnya kita butuh input untuk tanggal (current + 1 minggu),\n",
        "        #    tapi cara termudah adalah append dummy row dulu atau manfaatkan logika shift.\n",
        "        #    Disini kita memprediksi 'Next Step' berdasarkan data terakhir yang tersedia.\n",
        "\n",
        "        # Cara simpel: Ambil fitur dari baris terakhir data yang valid\n",
        "        last_row_features = full_features.iloc[[-1]][FEATURES]\n",
        "\n",
        "        # Namun wait, fitur lag_1 di baris terakhir adalah Quantity hari ini.\n",
        "        # Jadi baris terakhir full_features SUDAH mengandung data untuk prediksi besok (secara logic shift).\n",
        "        # TAPI, tanggalnya harus kita majukan.\n",
        "\n",
        "        # Logic Fix:\n",
        "        # Kita buat baris baru untuk tanggal yang mau diprediksi\n",
        "        next_date = current_input_data.index[-1] + pd.Timedelta(weeks=1)\n",
        "\n",
        "        # Buat dataframe sementara mencakup tanggal baru tersebut\n",
        "        # Isinya Quantity=0 dulu (dummy), nanti kita replace Lag-nya\n",
        "        next_row = pd.DataFrame({'Quantity': [0]}, index=[next_date])\n",
        "        temp_concat = pd.concat([current_input_data, next_row])\n",
        "\n",
        "        # Generate fitur ulang\n",
        "        feat_temp = create_features(temp_concat)\n",
        "\n",
        "        # Ambil X untuk tanggal target (baris paling bawah)\n",
        "        X_next = feat_temp.iloc[[-1]][FEATURES]\n",
        "\n",
        "        # 3. Predict\n",
        "        pred_value = model.predict(X_next)[0]\n",
        "        predictions.append(pred_value)\n",
        "\n",
        "        # 4. Update data history dengan hasil prediksi\n",
        "        #    Agar lag_1 untuk putaran berikutnya memakai hasil prediksi ini\n",
        "        #    Kita 'seolah-olah' menganggap prediksi ini adalah fakta\n",
        "        current_input_data = pd.concat([current_input_data, pd.DataFrame({'Quantity': [pred_value]}, index=[next_date])])\n",
        "\n",
        "    # --- EVALUASI ---\n",
        "    forecast_series = pd.Series(predictions, index=test_raw.index)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(test_raw['Quantity'], forecast_series))\n",
        "    mape = mean_absolute_percentage_error(test_raw['Quantity'], forecast_series)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'Category': category,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'Last_Actual': test_raw['Quantity'].iloc[-1],\n",
        "        'Last_Pred': forecast_series.iloc[-1]\n",
        "    })\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "    plt.plot(train_raw.index, train_raw['Quantity'], label='Train', color='tab:blue')\n",
        "    plt.plot(test_raw.index, test_raw['Quantity'], label='Test (Actual)', color='tab:green')\n",
        "    plt.plot(forecast_series.index, forecast_series, label='XGB Forecast', color='tab:red', linestyle='--', linewidth=2)\n",
        "\n",
        "    plt.title(f\"{category}\\nMAPE: {mape:.2%}\", fontsize=11, fontweight='bold')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "\n",
        "    if i == 0: plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- HASIL ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== TOP 5 BEST XGBOOST RESULTS ===\")\n",
        "print(df_eval[['Category', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage').head(53))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eiEPyI5XFwM4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiEPyI5XFwM4",
        "outputId": "0b474d80-7a11-4132-d85d-cbdc22da2a9c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "import optuna\n",
        "import warnings\n",
        "\n",
        "# Matikan log Optuna yang terlalu cerewet\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. DATA PREPARATION ---\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "test_weeks = 12\n",
        "evaluation_results = []\n",
        "\n",
        "print(f\"Running XGBoost + Optuna Tuning untuk {len(df_pivot.columns)} Kategori...\")\n",
        "print(\"Proses ini mungkin memakan waktu karena mencari parameter terbaik satu per satu...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 3. ADVANCED FEATURE ENGINEERING ---\n",
        "def create_features(df_input):\n",
        "    \"\"\"\n",
        "    Membuat fitur yang lebih kaya: Lags, Rolling Stats, dan Date Features\n",
        "    \"\"\"\n",
        "    df_input = df_input.copy()\n",
        "\n",
        "    # A. Date Features\n",
        "    df_input['week_of_year'] = df_input.index.isocalendar().week.astype(int)\n",
        "    df_input['month'] = df_input.index.month\n",
        "\n",
        "    # B. Lag Features (Masa Lalu)\n",
        "    for lag in [1, 2, 3, 4]:\n",
        "        df_input[f'lag_{lag}'] = df_input['Quantity'].shift(lag)\n",
        "\n",
        "    # C. Rolling Statistics (Tren & Volatilitas)\n",
        "    # Penting: Shift(1) dulu agar tidak bocor (data hari ini belum tahu rata-rata hari ini)\n",
        "    # Rata-rata 4 minggu terakhir\n",
        "    df_input['rolling_mean_4'] = df_input['Quantity'].shift(1).rolling(window=4).mean()\n",
        "    # Standar deviasi 4 minggu terakhir (mengukur kestabilan)\n",
        "    df_input['rolling_std_4'] = df_input['Quantity'].shift(1).rolling(window=4).std()\n",
        "\n",
        "    # D. Momentum (Perbedaan Lag)\n",
        "    # Apakah penjualan minggu lalu lebih tinggi dari 2 minggu lalu?\n",
        "    df_input['momentum_1_2'] = df_input['lag_1'] - df_input['lag_2']\n",
        "\n",
        "    return df_input\n",
        "\n",
        "# --- 4. OPTUNA OBJECTIVE FUNCTION ---\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Fungsi tujuan yang akan dioptimalkan oleh Optuna.\n",
        "    Mencari RMSE terkecil di data validasi.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), # L1 Regularization\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True), # L2 Regularization\n",
        "        'n_jobs': -1,\n",
        "        'objective': 'reg:squarederror',\n",
        "        'verbosity': 0\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**param)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict Validation Set (Standard prediction for speed in tuning)\n",
        "    preds = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "    return rmse\n",
        "\n",
        "# --- 5. MAIN LOOP ---\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(df_pivot.columns) / n_cols))\n",
        "\n",
        "plt.figure(figsize=(18, 5 * n_rows))\n",
        "\n",
        "FEATURES = ['week_of_year', 'month', 'lag_1', 'lag_2', 'lag_3', 'lag_4',\n",
        "            'rolling_mean_4', 'rolling_std_4', 'momentum_1_2']\n",
        "TARGET = 'Quantity'\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    # Setup Data\n",
        "    temp_df = df_pivot[[category]].rename(columns={category: 'Quantity'})\n",
        "\n",
        "    # Split Train & Test (Data Asli)\n",
        "    train_raw = temp_df.iloc[:-test_weeks].copy()\n",
        "    test_raw = temp_df.iloc[-test_weeks:].copy()\n",
        "\n",
        "    # --- PHASE A: HYPERPARAMETER TUNING (OPTUNA) ---\n",
        "    # Kita butuh Validation set untuk Optuna (ambil 20% terakhir dari training data)\n",
        "    # Agar Optuna tidak melihat Test Data sama sekali\n",
        "    val_size = int(len(train_raw) * 0.2)\n",
        "    train_optuna = train_raw.iloc[:-val_size]\n",
        "    val_optuna = train_raw.iloc[-val_size:]\n",
        "\n",
        "    # Create Features for Tuning\n",
        "    train_feat_opt = create_features(train_optuna).dropna()\n",
        "    val_feat_opt = create_features(pd.concat([train_optuna, val_optuna])).iloc[-val_size:]\n",
        "\n",
        "    X_t_opt = train_feat_opt[FEATURES]\n",
        "    y_t_opt = train_feat_opt[TARGET]\n",
        "    X_v_opt = val_feat_opt[FEATURES]\n",
        "    y_v_opt = val_feat_opt[TARGET]\n",
        "\n",
        "    # Run Optuna Study\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: objective(trial, X_t_opt, y_t_opt, X_v_opt, y_v_opt), n_trials=50) # 20 trials per category\n",
        "\n",
        "    best_params = study.best_params\n",
        "    print(f\"[{category}] Best Params: Depth={best_params['max_depth']}, LR={best_params['learning_rate']:.3f}\")\n",
        "\n",
        "    # --- PHASE B: FINAL TRAINING ---\n",
        "    # Gunakan parameter terbaik untuk training di FULL DATA TRAINING\n",
        "    train_features = create_features(train_raw).dropna()\n",
        "    X_train_full = train_features[FEATURES]\n",
        "    y_train_full = train_features[TARGET]\n",
        "\n",
        "    final_model = XGBRegressor(**best_params, n_jobs=-1, objective='reg:squarederror')\n",
        "    final_model.fit(X_train_full, y_train_full)\n",
        "\n",
        "    # --- PHASE C: RECURSIVE FORECASTING ---\n",
        "    current_input_data = train_raw.copy()\n",
        "    predictions = []\n",
        "\n",
        "    for week in range(len(test_raw)):\n",
        "        # 1. Update fitur\n",
        "        full_features = create_features(current_input_data)\n",
        "\n",
        "        # 2. Siapkan baris besok\n",
        "        next_date = current_input_data.index[-1] + pd.Timedelta(weeks=1)\n",
        "        next_row = pd.DataFrame({'Quantity': [0]}, index=[next_date])\n",
        "        temp_concat = pd.concat([current_input_data, next_row])\n",
        "\n",
        "        # 3. Generate fitur ulang (agar rolling & lag terupdate)\n",
        "        feat_temp = create_features(temp_concat)\n",
        "        X_next = feat_temp.iloc[[-1]][FEATURES]\n",
        "\n",
        "        # 4. Predict\n",
        "        pred_value = final_model.predict(X_next)[0]\n",
        "        predictions.append(pred_value)\n",
        "\n",
        "        # 5. Update History\n",
        "        current_input_data = pd.concat([current_input_data, pd.DataFrame({'Quantity': [pred_value]}, index=[next_date])])\n",
        "\n",
        "    # --- EVALUASI ---\n",
        "    forecast_series = pd.Series(predictions, index=test_raw.index)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(test_raw['Quantity'], forecast_series))\n",
        "    mape = mean_absolute_percentage_error(test_raw['Quantity'], forecast_series)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'Category': category,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'Best_Params': str(best_params)\n",
        "    })\n",
        "\n",
        "    # --- PLOTTING ---\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "    plt.plot(train_raw.index, train_raw['Quantity'], label='Train', color='tab:blue', alpha=0.5)\n",
        "    plt.plot(test_raw.index, test_raw['Quantity'], label='Actual', color='tab:green', linewidth=2)\n",
        "    plt.plot(forecast_series.index, forecast_series, label='XGB-Optuna', color='tab:red', linestyle='--', linewidth=2)\n",
        "\n",
        "    plt.title(f\"{category}\\nMAPE: {mape:.2%}\", fontsize=10, fontweight='bold')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "\n",
        "    if i == 0: plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- REPORT ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== TOP 5 BEST OPTIMIZED MODELS ===\")\n",
        "print(df_eval[['Category', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage').head(53))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ThyeyaEvhTgh",
      "metadata": {
        "id": "ThyeyaEvhTgh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. PREPARE WEEKLY DATA ---\n",
        "# (Sama seperti sebelumnya: Agregasi Mingguan & Interpolasi)\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "\n",
        "# Resample untuk memastikan urutan waktu lengkap\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "\n",
        "# Pivot: Baris=Tanggal, Kolom=Kategori\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TEST_WEEKS = 12       # Jumlah minggu yang akan diprediksi (Horizon)\n",
        "LOOK_BACK = 8         # Window size (Model melihat 8 minggu ke belakang untuk menebak minggu depan)\n",
        "\n",
        "print(f\"MODE: Sequence Modeling (Look-back: {LOOK_BACK} weeks)\")\n",
        "print(f\"Target Forecast: {TEST_WEEKS} weeks ahead\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "# --- 2. HELPER FUNCTION: CREATE INPUT SEQUENCES ---\n",
        "def create_sequences(data, look_back):\n",
        "    \"\"\"\n",
        "    Mengubah array time series menjadi format Supervised Learning (X, y)\n",
        "    Input: [10, 20, 30, 40, 50] dengan look_back=3\n",
        "    X: [[10, 20, 30], [20, 30, 40]]\n",
        "    y: [40, 50]\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i : i + look_back])\n",
        "        y.append(data[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# --- MAIN LOOP PER CATEGORY ---\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(df_pivot.columns) / n_cols))\n",
        "\n",
        "plt.figure(figsize=(18, 5 * n_rows))\n",
        "\n",
        "for i, category in enumerate(df_pivot.columns):\n",
        "\n",
        "    # Ambil data series per kategori\n",
        "    series_data = df_pivot[category].values\n",
        "\n",
        "    # Split Data: Train & Test (Raw Series)\n",
        "    train_series = series_data[:-TEST_WEEKS]\n",
        "    test_series = series_data[-TEST_WEEKS:]\n",
        "\n",
        "    # --- 3. TRAIN XGBOOST MODEL ON SEQUENCE DATA ---\n",
        "    # Ubah data training menjadi sequence (X, y)\n",
        "    X_train, y_train = create_sequences(train_series, LOOK_BACK)\n",
        "\n",
        "    # Define Model (Parameter standar yang robust untuk sequence)\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective='reg:squarederror',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit Model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # --- 4. GENERATE RECURSIVE SEQUENCE PREDICTIONS ---\n",
        "\n",
        "\n",
        "    # Inisialisasi input awal: Ambil 'look_back' data terakhir dari Training\n",
        "    current_batch = train_series[-LOOK_BACK:] # [t-8, ..., t-1]\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(TEST_WEEKS):\n",
        "        # Reshape data agar sesuai input model (1 sample, n features)\n",
        "        # current_batch shape: (8,) -> (1, 8)\n",
        "        input_seq = current_batch.reshape(1, LOOK_BACK)\n",
        "\n",
        "        # Prediksi 1 langkah ke depan\n",
        "        pred_value = model.predict(input_seq)[0]\n",
        "\n",
        "        # Simpan hasil\n",
        "        predictions.append(pred_value)\n",
        "\n",
        "        # Update Window (Sliding):\n",
        "        # Buang data terlama (index 0), masukkan prediksi baru ke paling belakang\n",
        "        current_batch = np.append(current_batch[1:], pred_value)\n",
        "\n",
        "    # --- 5. EVALUATE AND VISUALIZE ---\n",
        "    # Konversi ke Pandas Series untuk indexing tanggal\n",
        "    test_index = df_pivot.index[-TEST_WEEKS:]\n",
        "    forecast_series = pd.Series(predictions, index=test_index)\n",
        "\n",
        "    # Hitung Error\n",
        "    rmse = np.sqrt(mean_squared_error(test_series, forecast_series))\n",
        "    mape = mean_absolute_percentage_error(test_series, forecast_series)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'Category': category,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'Last_Actual': test_series[-1],\n",
        "        'Last_Pred': forecast_series.iloc[-1]\n",
        "    })\n",
        "\n",
        "    # Plotting\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "    # Plot Train (ambil sebagian akhir saja agar grafik tidak terlalu padat)\n",
        "    train_plot_idx = df_pivot.index[:-TEST_WEEKS]\n",
        "    plt.plot(train_plot_idx[-20:], train_series[-20:], label='History (Train)', color='tab:blue', alpha=0.5)\n",
        "\n",
        "    # Plot Test & Forecast\n",
        "    plt.plot(test_index, test_series, label='Actual (Test)', color='tab:green', linewidth=2)\n",
        "    plt.plot(test_index, forecast_series, label='Recursive Forecast', color='tab:red', linestyle='--', linewidth=2, marker='.')\n",
        "\n",
        "    plt.title(f\"{category}\\nMAPE: {mape:.2%}\", fontsize=10, fontweight='bold')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "\n",
        "    if i == 0: plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- REPORT ---\n",
        "df_eval = pd.DataFrame(evaluation_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== SEQUENCE MODEL PERFORMANCE ===\")\n",
        "print(df_eval[['Category', 'MAPE_Percentage']].sort_values(by='MAPE_Percentage'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756c9004",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80a42fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['PRODUCT_CATEGORY'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e4efb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_gow = df.copy()\n",
        "# df_gow2 = df_gow.copy()\n",
        "# df_gow = df_gow[(df_gow['PRODUCT_CATEGORY'] != 'ALAT BANGUNAN') & (df_gow['PRODUCT_CATEGORY'] != 'JAS HUJAN') & (df_gow['PRODUCT_CATEGORY'] != 'MAKANAN HEWAN')]\n",
        "# df = df_gow.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cfc588",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = df_gow2.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bUBCAf7hUVs",
      "metadata": {
        "id": "2bUBCAf7hUVs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. DATA PREPARATION ---\n",
        "# (Sama seperti sebelumnya)\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- CONFIG ---\n",
        "TEST_WEEKS = 12\n",
        "LOOK_BACK = 4 \n",
        "\n",
        "# --- 2. FITUR ENGINEERING (DENGAN LOG TRANSFORMATION) ---\n",
        "X_all = []\n",
        "y_all = []\n",
        "categories_list = []\n",
        "dates_list = []\n",
        "\n",
        "for category in df_pivot.columns:\n",
        "    series = df_pivot[category].values\n",
        "    \n",
        "    # === MODIFIKASI 1: LOG TRANSFORM PADA SERIES ===\n",
        "    # Kita log-kan seluruh data agar variansi mengecil\n",
        "    # np.log1p adalah log(x + 1) untuk handle nilai 0\n",
        "    series_log = np.log1p(series)\n",
        "    \n",
        "    for i in range(LOOK_BACK, len(series_log) - TEST_WEEKS + 1):\n",
        "        past_window = series_log[i-LOOK_BACK : i]\n",
        "        future_window = series_log[i : i+TEST_WEEKS] # Target juga dalam bentuk Log\n",
        "        \n",
        "        if len(future_window) < TEST_WEEKS:\n",
        "            continue\n",
        "            \n",
        "        feat_mean = np.mean(past_window)\n",
        "        feat_std = np.std(past_window)\n",
        "        \n",
        "        # Fitur sequence + statistik\n",
        "        features = list(past_window) + [feat_mean, feat_std]\n",
        "        \n",
        "        X_all.append(features)\n",
        "        y_all.append(future_window)\n",
        "        categories_list.append(category)\n",
        "        dates_list.append(df_pivot.index[i])\n",
        "\n",
        "X_df = pd.DataFrame(X_all)\n",
        "feat_cols = [f'Lag_{j}' for j in range(LOOK_BACK, 0, -1)] + ['Mean', 'Std']\n",
        "X_df.columns = feat_cols\n",
        "X_df['Category'] = categories_list\n",
        "X_df['Category'] = X_df['Category'].astype('category')\n",
        "X_df['Month'] = pd.to_datetime(dates_list).month\n",
        "\n",
        "y_arr = np.array(y_all)\n",
        "\n",
        "# --- 3. SPLIT TRAIN & TEST ---\n",
        "cutoff_date = df_pivot.index[-TEST_WEEKS]\n",
        "is_train = pd.to_datetime(dates_list) < cutoff_date\n",
        "is_test = pd.to_datetime(dates_list) == cutoff_date \n",
        "\n",
        "X_train = X_df[is_train]\n",
        "y_train = y_arr[is_train]\n",
        "X_test = X_df[is_test]\n",
        "y_test = y_arr[is_test] # Ini masih dalam bentuk LOG\n",
        "\n",
        "# --- 4. TRAINING ---\n",
        "# Kita naikkan sedikit learning rate karena Log space lebih sederhana\n",
        "lgbm = LGBMRegressor(n_estimators=500, learning_rate=0.07, num_leaves=20, n_jobs=-1)\n",
        "model = MultiOutputRegressor(lgbm)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. PREDICT & INVERSE TRANSFORM ---\n",
        "y_pred_log = model.predict(X_test)\n",
        "\n",
        "# === MODIFIKASI 2: KEMBALIKAN KE BENTUK ASLI (EXPM1) ===\n",
        "# y_test (actual) dan y_pred (prediction) dikembalikan ke satuan Quantity asli\n",
        "y_pred_final = np.expm1(y_pred_log)\n",
        "y_test_final = np.expm1(y_test)\n",
        "\n",
        "# --- 6. EVALUASI LENGKAP (MAPE & MAE) ---\n",
        "test_categories = X_test['Category'].values\n",
        "eval_results = []\n",
        "\n",
        "# Tampilkan 5 baris pertama untuk visualisasi\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "for i, cat_name in enumerate(test_categories):\n",
        "    actual = y_test_final[i]\n",
        "    pred = y_pred_final[i]\n",
        "    \n",
        "    # Paksa tidak negatif\n",
        "    pred = np.maximum(pred, 0)\n",
        "    \n",
        "    # Hitung Error\n",
        "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
        "    mape = mean_absolute_percentage_error(actual, pred)\n",
        "    mae = mean_absolute_error(actual, pred) # Tambahkan MAE\n",
        "    \n",
        "    # Logic untuk highlight\n",
        "    # Jika MAPE besar TAPI MAE kecil (misal beda cuma 2 pcs), tandai sebagai \"Low Volume Issue\"\n",
        "    note = \"\"\n",
        "    if mape > 0.5 and mae < 5:\n",
        "        note = \"(Low Vol)\"\n",
        "    \n",
        "    eval_results.append({\n",
        "        'Category': cat_name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,           # Penting buat Jas Hujan\n",
        "        'MAPE': mape,\n",
        "        'Note': note\n",
        "    })\n",
        "    \n",
        "    # Plot hanya 12 kategori pertama agar tidak berat\n",
        "    if i < 12:\n",
        "        plt.subplot(3, 4, i+1)\n",
        "        weeks = range(1, TEST_WEEKS+1)\n",
        "        plt.plot(weeks, actual, label='Actual', marker='o', color='green')\n",
        "        plt.plot(weeks, pred, label='Pred', marker='x', linestyle='--', color='red')\n",
        "        plt.title(f\"{cat_name}\\nMAPE: {mape:.1%} | MAE: {mae:.1f}\")\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- REPORT ---\n",
        "df_eval = pd.DataFrame(eval_results)\n",
        "df_eval['MAPE_Percentage'] = df_eval['MAPE'].apply(lambda x: f\"{x:.2%}\")\n",
        "print(\"\\n=== FINAL RESULTS (LOG TRANSFORMED) ===\")\n",
        "# Urutkan berdasarkan MAPE agar yang bagus di atas\n",
        "print(df_eval.sort_values(by='MAPE')[['Category', 'MAE', 'MAPE_Percentage', 'Note']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1890fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. DATA PREPARATION ---\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df_grouped = df.groupby([pd.Grouper(key='InvoiceDate', freq='W'), 'PRODUCT_CATEGORY'])['Quantity'].sum()\n",
        "df_wide = df_grouped.unstack(level='PRODUCT_CATEGORY').resample('W').asfreq()\n",
        "df_interpolated = df_wide.interpolate(method='linear')\n",
        "df = df_interpolated.stack().reset_index()\n",
        "df.columns = ['InvoiceDate', 'PRODUCT_CATEGORY', 'Quantity']\n",
        "df_pivot = df.pivot(index='InvoiceDate', columns='PRODUCT_CATEGORY', values='Quantity')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "FORECAST_WEEKS = 10 \n",
        "LOOK_BACK = 4\n",
        "\n",
        "# --- 2. TRAIN GLOBAL MODEL (LOG TRANSFORMED) ---\n",
        "X_all = []\n",
        "y_all = []\n",
        "categories_list = []\n",
        "\n",
        "for category in df_pivot.columns:\n",
        "    series = df_pivot[category].values\n",
        "    series_log = np.log1p(series)\n",
        "    \n",
        "    for i in range(LOOK_BACK, len(series_log) - FORECAST_WEEKS + 1):\n",
        "        past_window = series_log[i-LOOK_BACK : i]\n",
        "        future_window = series_log[i : i+FORECAST_WEEKS]\n",
        "        \n",
        "        if len(future_window) < FORECAST_WEEKS:\n",
        "            continue\n",
        "            \n",
        "        feat_mean = np.mean(past_window)\n",
        "        feat_std = np.std(past_window)\n",
        "        features = list(past_window) + [feat_mean, feat_std]\n",
        "        \n",
        "        X_all.append(features)\n",
        "        y_all.append(future_window)\n",
        "        categories_list.append(category)\n",
        "\n",
        "X_train_full = pd.DataFrame(X_all)\n",
        "feat_cols = [f'Lag_{j}' for j in range(LOOK_BACK, 0, -1)] + ['Mean', 'Std']\n",
        "X_train_full.columns = feat_cols\n",
        "X_train_full['Category'] = categories_list\n",
        "X_train_full['Category'] = X_train_full['Category'].astype('category')\n",
        "y_train_full = np.array(y_all)\n",
        "\n",
        "# Train\n",
        "lgbm = LGBMRegressor(n_estimators=1000, learning_rate=0.07, num_leaves=20, n_jobs=-1)\n",
        "model = MultiOutputRegressor(lgbm)\n",
        "model.fit(X_train_full, y_train_full)\n",
        "\n",
        "# --- 3. GENERATE PREDICTIONS ---\n",
        "last_date = df_pivot.index[-1]\n",
        "future_dates = pd.date_range(start=last_date + pd.Timedelta(weeks=1), periods=FORECAST_WEEKS, freq='W')\n",
        "future_forecasts = {}\n",
        "\n",
        "for category in df_pivot.columns:\n",
        "    latest_series_log = np.log1p(df_pivot[category].values[-LOOK_BACK:])\n",
        "    feat_mean = np.mean(latest_series_log)\n",
        "    feat_std = np.std(latest_series_log)\n",
        "    features = list(latest_series_log) + [feat_mean, feat_std]\n",
        "    \n",
        "    input_row = pd.DataFrame([features], columns=feat_cols)\n",
        "    input_row['Category'] = category\n",
        "    input_row['Category'] = input_row['Category'].astype('category')\n",
        "    \n",
        "    pred_log = model.predict(input_row)[0]\n",
        "    pred_final = np.expm1(pred_log)\n",
        "    future_forecasts[category] = np.maximum(pred_final, 0)\n",
        "\n",
        "# --- 4. PENGGABUNGAN DATA (MERGING) ---\n",
        "\n",
        "# A. Siapkan Data History (Actual)\n",
        "# Reset index agar tanggal jadi kolom, lalu melt agar jadi format panjang\n",
        "df_history_long = df_pivot.reset_index().melt(id_vars='InvoiceDate', var_name='PRODUCT_CATEGORY', value_name='Quantity')\n",
        "df_history_long['Data_Type'] = 'Actual'\n",
        "\n",
        "# B. Siapkan Data Forecast (Predicted)\n",
        "df_future_wide = pd.DataFrame(future_forecasts, index=future_dates)\n",
        "df_future_long = df_future_wide.reset_index().melt(id_vars='index', var_name='PRODUCT_CATEGORY', value_name='Quantity')\n",
        "df_future_long.rename(columns={'index': 'InvoiceDate'}, inplace=True)\n",
        "df_future_long['Data_Type'] = 'Forecast'\n",
        "\n",
        "# C. GABUNGKAN (CONCAT)\n",
        "df_combined = pd.concat([df_history_long, df_future_long], axis=0).sort_values(by=['PRODUCT_CATEGORY', 'InvoiceDate']).reset_index(drop=True)\n",
        "\n",
        "# Cek Hasil\n",
        "print(\"=== CONTOH DATA GABUNGAN (Head & Tail) ===\")\n",
        "print(df_combined.head(3))\n",
        "print(\"...\")\n",
        "print(df_combined.tail(3))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 5. VISUALISASI DARI DF GABUNGAN ---\n",
        "# Kita akan plot langsung dari df_combined\n",
        "categories_to_plot = df_combined['PRODUCT_CATEGORY'].unique()\n",
        "n_cols = 4\n",
        "n_rows = int(np.ceil(len(categories_to_plot) / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, category in enumerate(categories_to_plot):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Filter data per kategori\n",
        "    subset = df_combined[df_combined['PRODUCT_CATEGORY'] == category]\n",
        "    \n",
        "    # Pisahkan untuk pewarnaan\n",
        "    actual_data = subset[subset['Data_Type'] == 'Actual']\n",
        "    forecast_data = subset[subset['Data_Type'] == 'Forecast']\n",
        "    \n",
        "    # Agar garis nyambung, kita ambil titik terakhir Actual dan masukkan ke Forecast sebagai jembatan\n",
        "    last_actual = actual_data.iloc[-1]\n",
        "    connector = pd.DataFrame([last_actual])\n",
        "    # Ubah tipenya jadi Forecast agar ikut warna merah, tapi posisi tetap di tanggal terakhir actual\n",
        "    connector['Data_Type'] = 'Forecast' \n",
        "    forecast_plot_data = pd.concat([connector, forecast_data])\n",
        "    \n",
        "    # Plot Actual (Biru)\n",
        "    # Tampilkan 6 bulan terakhir data asli saja agar grafik tidak kekecilan\n",
        "    plot_start_date = actual_data['InvoiceDate'].max() - pd.Timedelta(weeks=24)\n",
        "    actual_plot = actual_data[actual_data['InvoiceDate'] >= plot_start_date]\n",
        "    \n",
        "    ax.plot(actual_plot['InvoiceDate'], actual_plot['Quantity'], label='Actual', color='tab:blue', marker='o', markersize=4)\n",
        "    \n",
        "    # Plot Forecast (Merah)\n",
        "    ax.plot(forecast_plot_data['InvoiceDate'], forecast_plot_data['Quantity'], label='Forecast', color='tab:red', linestyle='--', marker='x', markersize=4, linewidth=2)\n",
        "    \n",
        "    ax.set_title(f\"{category}\", fontsize=10, fontweight='bold')\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
        "    \n",
        "    if i == 0: ax.legend()\n",
        "\n",
        "# Hapus axes kosong\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 6. EXPORT / SIMPAN ---\n",
        "# df_combined siap disimpan ke Excel/CSV\n",
        "# df_combined.to_csv('Sales_Forecast_Combined.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c2a5a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_combined.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b8b403",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
